import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset, random_split
import torch.optim as optim
from preprocessing import preprocess_excel
from model.transformer_model import TransformerModel
import numpy as np

# === é…ç½®è¶…å‚æ•° ===
EPOCHS = 300
BATCH_SIZE = 32             # å¢å¤§æ‰¹é‡å¤§å°
LEARNING_RATE = 0.001       # å¢å¤§å­¦ä¹ ç‡
WEIGHT_DECAY = 0.005        # å‡å°L2æ­£åˆ™åŒ–å¼ºåº¦
DATA_PATH = "train_data.xlsx"
ADD_TIME_FEATURES = True
MODEL_SAVE_PATH = "transformer_model.pth"
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# === åŠ è½½å®Œæ•´æ•°æ®é›† ===
print("ğŸ“¥ åŠ è½½æ•°æ®ä¸­...")
dataset, scaler = preprocess_excel(DATA_PATH, add_time_features=ADD_TIME_FEATURES)

# === åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›† ===
train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_dataset, val_dataset = random_split(
    dataset, [train_size, val_size],
    generator=torch.Generator().manual_seed(42)
)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)

# === æ„é€ æ¨¡å‹ ===
input_dim = dataset[0][0].shape[-1]
model = TransformerModel(
    input_dim=input_dim,
    d_model=128,        # å¢åŠ åˆ°128
    nhead=8,           # å¢åŠ åˆ°8
    num_layers=3,      # å¢åŠ åˆ°3
    dim_feedforward=256, # å¢åŠ åˆ°256
    dropout=0.2        # é€‚å½“å‡å°dropout
).to(DEVICE)

# === ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ ===
optimizer = optim.AdamW(
    model.parameters(),
    lr=LEARNING_RATE,
    weight_decay=WEIGHT_DECAY
)
scheduler = optim.lr_scheduler.OneCycleLR(
    optimizer,
    max_lr=0.001,
    epochs=EPOCHS,
    steps_per_epoch=len(train_loader),
    pct_start=0.3,
    anneal_strategy='cos'
)




# === è‡ªå®šä¹‰å¤åˆæŸå¤±å‡½æ•° ===
def composite_loss(pred, target, alpha=0.6, beta=0.4, epsilon=1e-3):
    # å¢åŠ MSEçš„æƒé‡
    mse = F.mse_loss(pred, target)
    denom = torch.clamp(torch.abs(target), min=epsilon)
    mape = torch.mean(torch.abs((pred - target) / denom))
    return alpha * mse + beta * mape



# === è¯„ä¼°å‡½æ•° ===
def evaluate(model, data_loader):
    model.eval()
    total_loss = 0
    with torch.no_grad():
        for X, Y in data_loader:
            X, Y = X.to(DEVICE), Y.to(DEVICE)
            preds = model(X)
            loss = composite_loss(preds, Y)
            total_loss += loss.item() * X.size(0)
    return total_loss / len(data_loader.dataset)


# === è®­ç»ƒè¿‡ç¨‹ ===
best_val_loss = float("inf")
patience = 15  # å‡å°‘æ—©åœè½®æ•°
patience_counter = 0
prev_lr = LEARNING_RATE
train_losses = []
val_losses = []

print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
for epoch in range(EPOCHS):
    # è®­ç»ƒé˜¶æ®µ
    model.train()
    train_loss = 0.0

    for X_batch, Y_batch in train_loader:
        X_batch, Y_batch = X_batch.to(DEVICE), Y_batch.to(DEVICE)

        optimizer.zero_grad()
        preds = model(X_batch)
        loss = composite_loss(preds, Y_batch)
        loss.backward()

        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        optimizer.step()
        train_loss += loss.item() * X_batch.size(0)

    avg_train_loss = train_loss / len(train_loader.dataset)

    # éªŒè¯é˜¶æ®µ
    avg_val_loss = evaluate(model, val_loader)

    train_losses.append(avg_train_loss)
    val_losses.append(avg_val_loss)

    print(f"Epoch {epoch + 1}/{EPOCHS}")
    print(f"è®­ç»ƒæŸå¤±: {avg_train_loss:.6f}")
    print(f"éªŒè¯æŸå¤±: {avg_val_loss:.6f}")

    # æ›´æ–°å­¦ä¹ ç‡
    scheduler.step(avg_val_loss)

    # æ£€æŸ¥å­¦ä¹ ç‡å˜åŒ–
    current_lr = optimizer.param_groups[0]['lr']
    if current_lr != prev_lr:
        print(f"ğŸ“‰ Learning rate adjusted to: {current_lr:.6f}")
        prev_lr = current_lr

    # æ—©åœæ£€æŸ¥
    if avg_val_loss < best_val_loss:
        best_val_loss = avg_val_loss
        patience_counter = 0
        torch.save(model.state_dict(), MODEL_SAVE_PATH)
        print(f"âœ… æœ€ä½³æ¨¡å‹å·²ä¿å­˜ï¼ŒéªŒè¯æŸå¤±={best_val_loss:.6f}")
    else:
        patience_counter += 1
        if patience_counter >= patience:
            print(f"âš ï¸ æ—©åœè§¦å‘ï¼{patience} ä¸ª epoch éªŒè¯æŸå¤±æœªæ”¹å–„")
            break

print("ğŸ‰ è®­ç»ƒå®Œæˆã€‚æ¨¡å‹ä¿å­˜è‡³:", MODEL_SAVE_PATH)

# === ç»˜åˆ¶æŸå¤±æ›²çº¿ ===
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.plot(train_losses, label='Training Loss')
plt.plot(val_losses, label='Validation Loss')
plt.title('Training and Validation Losses')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()